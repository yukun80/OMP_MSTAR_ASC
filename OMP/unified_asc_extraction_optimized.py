"""
ä¼˜åŒ–ç‰ˆç»Ÿä¸€ASCæå–ç®—æ³• - è§£å†³å‚æ•°é…ç½®é—®é¢˜
===============================================

åŸºäºå¯¹æ¯”æµ‹è¯•çš„ç»“æœï¼Œä¼˜åŒ–ç®—æ³•å‚æ•°ï¼š
1. é™ä½adaptive_thresholdé¿å…è¿‡æ—©åœæ­¢
2. å‡å°‘position_samplesç¼©å°å­—å…¸è§„æ¨¡
3. å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple
import time

from asc_extraction_fixed_v2 import ASCExtractionFixedV2, verify_coordinate_system, visualize_extraction_results
from demo_high_precision import find_best_mstar_file


class OptimizedUnifiedASCExtractor:
    """
    ä¼˜åŒ–ç‰ˆç»Ÿä¸€ASCæå–å™¨ - å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡
    """

    def __init__(
        self,
        image_size: Tuple[int, int] = (128, 128),
        max_scatterers: int = 20,
        adaptive_threshold: float = 0.02,  # é™ä½é˜ˆå€¼ï¼Œé¿å…è¿‡æ—©åœæ­¢
        max_iterations: int = 30,
        position_samples: int = 20,  # å‡å°‘é‡‡æ ·ï¼Œå¹³è¡¡æ•ˆç‡
        enable_progressive_sampling: bool = True,  # å¯ç”¨æ¸è¿›å¼é‡‡æ ·
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆç»Ÿä¸€æå–å™¨
        """
        self.image_size = image_size
        self.max_scatterers = max_scatterers
        self.adaptive_threshold = adaptive_threshold
        self.max_iterations = max_iterations
        self.position_samples = position_samples
        self.enable_progressive_sampling = enable_progressive_sampling

        print(f"ğŸš€ ä¼˜åŒ–ç‰ˆç»Ÿä¸€ASCæå–å™¨åˆå§‹åŒ–")
        print(f"   ç®—æ³•ç­–ç•¥: å•é˜¶æ®µå…¨å‚æ•°è¿­ä»£æå– (ä¼˜åŒ–ç‰ˆ)")
        print(f"   è‡ªé€‚åº”é˜ˆå€¼: {adaptive_threshold} (æ›´å®½æ¾)")
        print(f"   ä½ç½®é‡‡æ ·: {position_samples}Ã—{position_samples} (ä¼˜åŒ–æ•ˆç‡)")
        print(f"   æ¸è¿›å¼é‡‡æ ·: {'å¯ç”¨' if enable_progressive_sampling else 'ç¦ç”¨'}")

    def create_optimized_extractor(self) -> ASCExtractionFixedV2:
        """
        åˆ›å»ºä¼˜åŒ–çš„æ ¸å¿ƒæå–å™¨
        """
        if self.enable_progressive_sampling:
            # æ¸è¿›å¼é…ç½®ï¼šä»ç®€å•åˆ°å¤æ‚
            return ASCExtractionFixedV2(
                image_size=self.image_size,
                extraction_mode="progressive",
                adaptive_threshold=self.adaptive_threshold,
                max_iterations=self.max_iterations,
                max_scatterers=self.max_scatterers,
                # ä¼˜åŒ–çš„å‚æ•°é…ç½®ï¼šå¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡
                alpha_values=[-1.0, -0.5, 0.0, 0.5, 1.0],  # ä¿æŒå…¨å‚æ•°
                length_values=[0.0],  # ç®€åŒ–é•¿åº¦å‚æ•°
                phi_bar_values=[0.0, np.pi / 2],  # ç®€åŒ–æ–¹å‘è§’
                position_samples=self.position_samples,  # è¾ƒå°çš„é‡‡æ ·å¯†åº¦
            )
        else:
            # æ ‡å‡†é…ç½®
            return ASCExtractionFixedV2(
                image_size=self.image_size,
                extraction_mode="progressive",
                adaptive_threshold=self.adaptive_threshold,
                max_iterations=self.max_iterations,
                max_scatterers=self.max_scatterers,
                position_samples=self.position_samples,
            )

    def extract_scatterers_optimized(self, complex_image: np.ndarray) -> List[Dict]:
        """
        æ‰§è¡Œä¼˜åŒ–çš„ASCæ•£å°„ä¸­å¿ƒæå–
        """
        print("\n" + "=" * 70)
        print("ğŸ¯ ä¼˜åŒ–ç‰ˆç»Ÿä¸€ASCæå– - å¹³è¡¡ç²¾åº¦ä¸æ•ˆç‡")
        print("=" * 70)

        # 1. åˆ›å»ºä¼˜åŒ–çš„æ ¸å¿ƒæå–å™¨
        print("\nğŸ”§ åˆå§‹åŒ–ä¼˜åŒ–çš„æ ¸å¿ƒæå–å™¨...")
        core_extractor = self.create_optimized_extractor()

        # 2. åæ ‡ç³»éªŒè¯
        print("\nğŸ” éªŒè¯åæ ‡ç³»ä¿®å¤...")
        if not verify_coordinate_system(core_extractor):
            print("âŒ åæ ‡ç³»éªŒè¯å¤±è´¥")
            return []
        print("âœ… åæ ‡ç³»éªŒè¯é€šè¿‡")

        # 3. æ‰§è¡Œæ ¸å¿ƒæå–ç®—æ³•
        print("\nğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆå•é˜¶æ®µæå–...")
        print("   æ”¹è¿›: é™ä½é˜ˆå€¼ + ç¼©å°å­—å…¸ + ä¿æŒå…¨å‚æ•°æ”¯æŒ")

        start_time = time.time()
        scatterers = core_extractor.extract_asc_scatterers_v2(complex_image)
        extraction_time = time.time() - start_time

        # 4. ç»“æœåˆ†æ
        print(f"\nğŸ“Š ä¼˜åŒ–ç‰ˆæå–å®Œæˆ (è€—æ—¶: {extraction_time:.2f}s)")

        if not scatterers:
            print("âŒ æœªæå–åˆ°ä»»ä½•æ•£å°„ä¸­å¿ƒ")
            return []

        # æŒ‰å¹…åº¦æ’åº
        scatterers.sort(key=lambda s: s["estimated_amplitude"], reverse=True)

        # è¯¦ç»†åˆ†æ
        self._analyze_optimized_results(scatterers, extraction_time)

        return scatterers

    def _analyze_optimized_results(self, scatterers: List[Dict], extraction_time: float):
        """
        åˆ†æä¼˜åŒ–ç‰ˆç®—æ³•çš„ç»“æœ
        """
        print(f"\nğŸ“ˆ ä¼˜åŒ–ç‰ˆç»“æœåˆ†æ:")
        print(f"   æ•£å°„ä¸­å¿ƒæ€»æ•°: {len(scatterers)}")
        print(f"   æå–æ•ˆç‡: {extraction_time:.2f}s")

        # æ•£å°„ç±»å‹åˆ†å¸ƒ
        type_dist = {}
        alpha_dist = {}
        opt_success_count = 0

        for sc in scatterers:
            stype = sc["scattering_type"]
            alpha = sc["alpha"]
            type_dist[stype] = type_dist.get(stype, 0) + 1
            alpha_dist[alpha] = alpha_dist.get(alpha, 0) + 1
            if sc.get("optimization_success", False):
                opt_success_count += 1

        print(f"   æ•£å°„ç±»å‹åˆ†å¸ƒ: {type_dist}")
        print(f"   Î±å‚æ•°åˆ†å¸ƒ: {alpha_dist}")
        print(f"   ä¼˜åŒ–æˆåŠŸç‡: {opt_success_count}/{len(scatterers)} ({opt_success_count/len(scatterers)*100:.1f}%)")

        # ä¸ç†è®ºæœŸæœ›å¯¹æ¯”
        self._assess_optimization_effectiveness(scatterers, extraction_time)

    def _assess_optimization_effectiveness(self, scatterers: List[Dict], extraction_time: float):
        """
        è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
        """
        print(f"\nğŸ† ä¼˜åŒ–æ•ˆæœè¯„ä¼°:")

        # æ•°é‡è¯„ä¼°
        if len(scatterers) >= 10:
            print(f"   âœ… æ•£å°„ä¸­å¿ƒæ•°é‡æ”¹å–„ ({len(scatterers)}ä¸ª)")
        elif len(scatterers) >= 5:
            print(f"   âš ï¸ æ•£å°„ä¸­å¿ƒæ•°é‡ä¸€èˆ¬ ({len(scatterers)}ä¸ª)")
        else:
            print(f"   âŒ æ•£å°„ä¸­å¿ƒæ•°é‡ä»ç„¶åå°‘ ({len(scatterers)}ä¸ª)")

        # æ•ˆç‡è¯„ä¼°
        if extraction_time < 60:
            print(f"   âœ… è®¡ç®—æ•ˆç‡è‰¯å¥½ ({extraction_time:.1f}s)")
        elif extraction_time < 120:
            print(f"   âš ï¸ è®¡ç®—æ•ˆç‡ä¸€èˆ¬ ({extraction_time:.1f}s)")
        else:
            print(f"   âŒ è®¡ç®—æ•ˆç‡åä½ ({extraction_time:.1f}s)")

        # æ•£å°„ç±»å‹å¤šæ ·æ€§è¯„ä¼°
        strong_types = sum(1 for s in scatterers if s["alpha"] in [-1.0, -0.5])
        strong_ratio = strong_types / len(scatterers) if scatterers else 0

        if strong_ratio > 0.6:
            print(f"   âœ… å¼ºæ•£å°„ç±»å‹è¯†åˆ«èƒ½åŠ›å¼º ({strong_ratio:.1%})")
        elif strong_ratio > 0.3:
            print(f"   âš ï¸ å¼ºæ•£å°„ç±»å‹è¯†åˆ«èƒ½åŠ›ä¸€èˆ¬ ({strong_ratio:.1%})")
        else:
            print(f"   âŒ å¼ºæ•£å°„ç±»å‹è¯†åˆ«èƒ½åŠ›å¼± ({strong_ratio:.1%})")


def compare_with_baseline():
    """
    ä¸åŸºå‡†ç®—æ³•è¿›è¡Œå¿«é€Ÿå¯¹æ¯”
    """
    print("ğŸ”¬ ä¼˜åŒ–ç‰ˆç®—æ³•å¿«é€ŸéªŒè¯")
    print("=" * 50)

    # æ•°æ®å‡†å¤‡
    mstar_file = find_best_mstar_file()
    if not mstar_file:
        print("âŒ æ— æ³•æ‰¾åˆ°MSTARæ•°æ®æ–‡ä»¶")
        return

    # åˆ›å»ºä¼˜åŒ–ç‰ˆæå–å™¨
    extractor = OptimizedUnifiedASCExtractor(
        max_scatterers=20,
        adaptive_threshold=0.02,  # æ›´å®½æ¾çš„é˜ˆå€¼
        position_samples=20,  # æ›´å°çš„å­—å…¸
        enable_progressive_sampling=True,
    )

    # åŠ è½½æ•°æ®
    core_extractor = extractor.create_optimized_extractor()
    magnitude, complex_image = core_extractor.load_mstar_data_robust(mstar_file)

    # æ‰§è¡Œæå–
    scatterers = extractor.extract_scatterers_optimized(complex_image)

    # å¿«é€ŸéªŒè¯
    if scatterers:
        print(f"\nâœ… ä¼˜åŒ–ç‰ˆç®—æ³•æˆåŠŸæå– {len(scatterers)} ä¸ªæ•£å°„ä¸­å¿ƒ")

        # ç®€å•çš„ç›®æ ‡åŒºåŸŸæ£€æµ‹
        magnitude = np.abs(complex_image)
        max_val = np.max(magnitude)
        threshold = max_val / 10
        high_intensity_mask = magnitude > threshold
        rows, cols = np.where(high_intensity_mask)

        if len(rows) > 0:
            img_h, img_w = complex_image.shape
            x_min = (np.min(cols) / img_w) * 2 - 1
            x_max = (np.max(cols) / img_w) * 2 - 1
            y_min = (np.min(rows) / img_h) * 2 - 1
            y_max = (np.max(rows) / img_h) * 2 - 1

            # æ£€æŸ¥ç›®æ ‡åŒ¹é…åº¦
            in_target = 0
            for sc in scatterers:
                x, y = sc["x"], sc["y"]
                if x_min <= x <= x_max and y_min <= y <= y_max:
                    in_target += 1

            match_ratio = in_target / len(scatterers)
            print(f"ğŸ¯ ç›®æ ‡åŒºåŸŸåŒ¹é…åº¦: {in_target}/{len(scatterers)} ({match_ratio:.1%})")

            if match_ratio > 0.5:
                print("   âœ… ä¼˜åŒ–æˆåŠŸï¼æ˜¾è‘—æ”¹å–„äº†ç›®æ ‡åŒ¹é…åº¦")
            elif match_ratio > 0.2:
                print("   âš ï¸ æœ‰æ‰€æ”¹å–„ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                print("   âŒ ç›®æ ‡åŒ¹é…åº¦æ”¹å–„æœ‰é™")

        # å¯è§†åŒ–ç»“æœ
        visualize_extraction_results(complex_image, scatterers, save_path="optimized_unified_result.png")
        print("âœ… ç»“æœå·²ä¿å­˜åˆ°: optimized_unified_result.png")
    else:
        print("âŒ ä¼˜åŒ–ç‰ˆç®—æ³•æœªæå–åˆ°æ•£å°„ä¸­å¿ƒ")


def main():
    """
    ä¸»è¿è¡Œå‡½æ•°
    """
    print("ğŸ¯ ä¼˜åŒ–ç‰ˆç»Ÿä¸€ASCæå–ç®—æ³•")
    print("=" * 70)
    print("ä¼˜åŒ–ç­–ç•¥:")
    print("  ğŸ”§ é™ä½adaptive_threshold (0.05 â†’ 0.02)")
    print("  ğŸ”§ å‡å°‘position_samples (32 â†’ 20)")
    print("  ğŸ”§ ç®€åŒ–å­—å…¸å‚æ•°ç»„åˆ")
    print("  âœ… ä¿æŒå…¨å‚æ•°æ”¯æŒé¿å…æ¨¡å‹å¤±é…")
    print("=" * 70)

    compare_with_baseline()


if __name__ == "__main__":
    main()
